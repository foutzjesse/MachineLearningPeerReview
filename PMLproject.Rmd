---
title: "PMLproject"
author: "Jesse Foutz"
date: "October 8, 2016"
output: html_document
---
Analysis of Biometrics Data
=========================================================
This report assesses data collected from personal activity monitoring devices such as the *Jawbone Up, Nike FuelBand,* and *Fitbit* for barbell lifts from six participants. The report will use machine learning principles to predict whether the exercises were performed correctly or incorrectly by participants.

```{r}
library(caret)
library(randomForest)
library(e1071)
library(rpart)
library(rpart.plot)
```

##Training Data
The training data was retrieved from [this website](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).
```{r}
training <- read.csv("pml-training.csv")
dim(training)
```

This is a very large data set--nearly 20,000 rows with 160 columns in each. Not all this data will be useful, such as the row-number column, the participant name, and the timestamps. We will remove those columns.
```{r}
training <- subset(training, select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
```

Some columns contain near-zero variance data, and some contain very little data at all. The near-zero variance variables and variables with less than half of their records populated will be removed.
```{r}
training <- subset(training, select=-nearZeroVar(training))
training <- training[,colSums(is.na(training))/nrow(training) <= .5]
```

##Test Data
The test data comes from [this website](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
```{r}
test <- read.csv("pml-testing.csv")
dim(test)
```

This data set is much smaller; we only need to remove the columns to make it match the training dataset.
```{r}
test <- subset(test, select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
test <- subset(test, select=-nearZeroVar(test))
test <- test[,colSums(is.na(test))/nrow(test) <= .5]
```

##Information
According to [the writeup](http://groupware.les.inf.puc-rio.br/har) for this data set, "six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

We will use the training data's classe variable as the outcome and use the **random forest** and **decision tree** methods to predict the level corresponding to each observation in the test data.

##Data Manipulation
Cross validation will be performed to analyze the data. The training data will be itself broken into testing and training subsets in a 1:3 ratio. The date at the time of writing will be used as a seed.
```{r}
set.seed(20161008)
trainingPartition <- createDataPartition(training$classe, p=.75, list=F)
trainingTraining <- training[trainingPartition,]
trainingTest <- training[-trainingPartition,]
```

Let's look at the distribution of outcomes in the training-training subset.
```{r}
plot(trainingTraining$classe, ylab="Frequency")
```

The levels are statistically quite close--less than an order of magnitude apart. Level A (proper execution) is most frequent, and Level D (lifting the dumbbell halfway) is rarest.

##First Model: Random Forest
First, a model will be built by fitting the random forest model to the training data.
```{r}
rfModel <- randomForest(classe ~ ., data=trainingTraining, method="class")
```

The in-sample error will be calculated by fitting the model to the test data.
```{r}
prediction <- predict(rfModel, trainingTest, type="class")
```

We will test the results with a confusion matrix:
```{r}
confusionMatrix(prediction, trainingTest$classe)
```

**The accuracy of the random forest test was 0.9967.**

##Second Model: Decision Tree
We will do the same thing using a decision tree.
```{r}
dtModel <- rpart(classe ~ ., data=trainingTraining, method="class")
prediction <- predict(dtModel, trainingTest, type="class")
```

The resulting tree looks like this:
```{r}
rpart.plot(dtModel, extra = 102, under = T, faclen = 0)
```

We will now check the results with a confusion matrix.
```{r}
confusionMatrix(prediction, trainingTest$classe)
```

**The accuracy of the decision tree is 0.7447,** much less than that of the random forest.

##Prediction
We will now conduct our prediction, using the superior decision tree model.
```{r}
prediction <- predict(rfModel, test)
```

##Result
```{r}
plot(prediction, main = "Prediction Results", ylab="Frequency")
prediction
```
The prediction shows that **the most common execution was done while throwing the elbows to the front, followed by proper execution.**